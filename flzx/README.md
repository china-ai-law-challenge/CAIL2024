# 法律咨询
## 任务介绍
该赛道由山东大学牵头承办。法律咨询是一种帮助客户解决法律问题的服务。客户可能会咨询各种法律问题，包括但不限于合同纠纷、劳动法问题、知识产权保护、家庭法律事务等。参赛者需要模拟客户与律师之间的交流，反映出真实的公众法律咨询场景。

本任务旨在通过构建人工智能模型，实现常见法律咨询的自动化问答，能够根据复杂的多轮对话上下文，提供清晰、准确的法律建议或问题解答。

## 数据集介绍

在本任务的初赛阶段，我们提供多轮对话的训练集和测试集。训练集包含100个咨询案例，测试集则包含200个咨询案例。每个案例均围绕单一主题展开，并且模拟了两名角色之间的多轮对话：一名角色为咨询者，负责提出问题；另一名角色为法律专家，负责根据咨询者的问题提供专业的法律解答。复赛阶段、封闭评测阶段将使用不公开的数据进行评测，数据结构、数据类型与初赛阶段类似。以下是一条数据样例：

```json
[{
    "id": 0,
    "conversation":[ 
    {
        "role": "user",
        "content": "如果夫妻一方因为赌博而离婚，可否让其净身出户"
    },
    {
        "role": "assistant",
        "content": "根据您提供的情况，如果夫妻一方因为赌博而离婚，那么在离婚分割夫妻共同财产时，另一方可以根据《民法典》第一千零九十二条的规定，请求法院少分或者不分涉及赌博产生的财产份额。但是需要注意的是，这个规定只适用于那些隐藏、转移、变卖、毁损、挥霍夫妻共同财产，或者伪造夫妻共同债务企图侵占另一方财产的行为。如果另一方不能证明这种行为的存在，那么是不能净身出户的。\n\n另外，离婚时，夫妻的共同财产应由双方协议处理，协议不成的，由人民法院根据财产的具体情况，按照照顾子女、女方和无过错方权益的原则判决，根据实际情况进行分割。\n\n最后，如果离婚是因为夫妻一方有重大过错，无过错方有权请求损害赔偿。但是需要特别说明的是，《民法典》第一千零九十一条仅列举了一些重大过错的情形，是否构成重大过错应根据实际情况进行分析。您提供的情况中并没有明确另一方是否有重大过错，因此需要了解更多事实后来判断。请问另一方是否有重大过错？"
    },
    {
        "role": "user",
        "content": "是有重大过错。"
    },
    {
        "role": "assistant",
        "content": "如果您所说的另一方确实有重大过错，那么根据《民法典》第一千零九十一条的规定，您可以请求对方进行损害赔偿。但是需要注意的是，是否构成重大过错取决于具体事实，请您提供更多证据和事实以便我们更好地为您提供法律意见。另外，夫妻离婚时还需要考虑子女抚养、财产分割等问题，请您做好相关准备以应对可能出现的情况。"
    }]
}]
```
复赛和封测阶段将会提供更具有挑战性的数据。

## 评价方式

本任务将从两个维度进行评测，即客观评测指标和主观评测指标：

客观评测指标包括法律正确性评价以及语义相似度评价。其中，法律正确性评价将采用ROUGE(Recall-Oriented Understudy for Gisting Evaluation)评测指标，通过比较模型生成回答与参考答案之间的文本差异，测量评估回答中关键关键信息的正确性；语义相似度评价将采用词向量指标（如BERTScore）来评测生成回答与提供参考答案之间的语义一致性。

主观评测指标包括生成对话的连贯性以及法律知识的准确性。生成对话的连贯性衡量生成的答案是否与对话上下文在话题上保持相关性，语言表达是否连贯通顺，用户对对话是否满意。法律知识的准确性是回答引用的法律知识和法律条文是否准备。初赛阶段该指标由大模型进行模拟打分。

考虑到过长回答可能会影响大模型的评分结果，得分将会对过长答案进行一定的惩罚，具体方式是：对于超过标准答案（reference）长度两倍的回答将最终得分乘以惩罚系数0 < delta < 1 

最终得分将由客观评测指标得分与主观评测指标得分加权得到

## 结果提交形式
提交结果统一命名为`prediction.json`。提交格式参考`prediction.json`，其中每一行为一个字典，对应一个测试样例的答案。

## 基线系统
`baseline.py`中提供了一个基于`chatglm4`的代码实现，输入格式参考`test_data.json`，输出格式参考`prediction.json`

**基准算法性能：** 
```json
{
    "rouge-l": 0.2056,
    "BertScore": 0.6865,
    "llm_score": 0.4370
}
```
最终 `rouge-l / 3 + BertScore / 3 + llm_score / 3 = 0.443034429`，（超过此分数的队伍进入复赛阶段）

**复赛评测指标：**

基于初赛的经验，我们在复赛设计了更科学的评测指标，现说明如下：

`score = 0.3 * rouge_score + 0.2 * bert_score + 0.3 * llm_score + 0.2 * key_word_score`

- rouge_score: 评价**法律正确性**，通过比较模型生成回答与参考答案之间的文本差异，测量评估回答中关键关键信息的正确性。
- bert_score: 评价**语义相似度**，采用词向量指标来评测生成回答与提供参考答案之间的语义一致性。
- llm_score: 评价**对话的连贯性以及法律知识的准确性**。生成对话的连贯性衡量生成的答案是否与对话上下文在话题上保持相关性，语言表达是否连贯通顺，用户对对话是否满意。法律知识的准确性是回答引用的法律知识和法律条文是否准备。该指标由由大模型进行打分，考虑到过长回答可能会影响大模型的评分结果，得分将会对过长答案进行一定的惩罚，具体方式是：对于超过标准答案（reference）长度两倍的回答将最终得分乘以惩罚系数 delta = 0.5
- key_word_score: 评价**法律要素的全面性和准确性**，即考核选手的答案中是否包含了**关键、精确**的法律要素，包括回答所依赖的**具体法律条文**，如《中华人民共和国xx法》第x条，以及**案件相关的关键信息**。该指标将提取选手提交答案中的关键词并与 ground_truth 中的关键词进行比对，全部正确得1分，部分正确按比例给分。

最终成绩由上述分数加权得到。

## 赛程赛制
- 9月13日至10月18日，初赛阶段。
- 10月18日至11月15日，复赛阶段。
- 11月16日至11月31日，封测阶段。

1. 初赛阶段：开启本任务比赛报名，提供小规模数据集，用于编写模型进行训练和测试，选手需在小规模数据上表现超过官方baseline，才能进入复赛。

2. 复赛阶段：开放第二阶段测试。对于高于任务预设基准算法成绩的队伍，我们将提供全量数据并开放第二阶段的测试提交。

3. 封测阶段：封闭评测。第二阶段结束时，所有参赛者需要提交最终模型或调用接口，并在封闭的新测试集上进行测试。