# 方法介绍
根据训练数据构建出的指令微调数据集微调法律大语言模型，例如：
```
{"instruction": "假设你是法律领域专家。请根据法律案件的事实部分给出其司法判决说理内容。", "input": "事实部分：一个案件事实。 判决说理：", "answer": "判决说理部分：一个判决说理。"}
```
# 使用方法
该项目使用微调的[lexilaw-6b](https://github.com/CSHaitao/LexiLaw)的LexiLaw_Finetune作为基线模型.
您需要先获取LexiLaw项目文件以及模型参数文件，并且使用pip install -r requirements.txt安装配置项目环境。然后运行process2train.py，将比赛训练数据集加工成指令微调数据集格式, 并使用freeze_single.sh脚本使用构建好的指令微调数据集微调模型。最后将微调好的大语言模型在测试集上进行测试，得到最后的预测结果。
# 硬件配置
该项目的微调法律大语言模型在单块A600 上训练，cuda版本为12.0，该方法的训练过程需要35G显存，但在推理阶段仅需要24G显存。
# 结果
```
{"EM": 0.26}
{"F1": 0.4856665432232515}
{"ROUGE": 0.15085959212636374}
{"LLM": 71.15333333333334}
{"macro-F1": 0.14468202028819765}
```
最终 0.15 * EM + 0.3 * F1 + 0.35 * ROUGE + 0.1 * LLM /100 + 0.1 * MACRI-F1
最终结果为：0.32312235557335584